# Outlier Sensitivity Experiment

The experiment was done on Iris.csv data, for 3 different versions of PCA implemented in pca1.py, pca2.py, pca3.py. Instead of manually checking the difference for all data points, the funcion find_most_influential_point() automatically calculates the sum of squared differences between 2d representation of full data and 2d representation of data without x_i. The implementation of the function is below. Any of the 3 PCA implementations can be plugged as simple_pca_2d(data) in the code. 


# The Implementation

def find_most_influential_point(original_csv, pca_2d_csv):
    data = load_data(original_csv)
    reference_2d = load_data(pca_2d_csv)

    n_samples = data.shape[0]
    max_diff = -1
    influential_index = -1

    for i in range(n_samples):
        # Leave out i-th row
        data_reduced = np.delete(data, i, axis=0)

        # Perform PCA on reduced data
        projected_reduced = simple_pca_2d(data_reduced)

        # Compare with reference 2D (skip the removed point)
        reference_reduced = np.delete(reference_2d, i, axis=0)

        # Sum of squared differences
        diff = np.sum((projected_reduced - reference_reduced) ** 2)

        if diff > max_diff:
            max_diff = diff
            influential_index = i

    print(f"Most influential data point is at index {influential_index} with difference {max_diff}")
    return influential_index



# Result

for pca1:
Most influential data point is at index 149 with difference 0.3041055251213149

for pca2:
Most influential data point is at index 149 with difference 37.334322306925564

for pca3:
Most influential data point is at index 0 with difference 0.028113616049567525